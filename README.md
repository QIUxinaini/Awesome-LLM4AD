# Awesome-LLM-for-Autonomous-Driving-Resources
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)![GitHub stars](https://img.shields.io/github/stars/Thinklab-SJTU/Awesome-LLM-for-Autonomous-Driving-Resources?color=yellow) ![GitHub forks](https://img.shields.io/github/forks/Thinklab-SJTU/Awesome-LLM-for-Autonomous-Driving-Resources?color=9cf) [![GitHub license](https://img.shields.io/github/license/Thinklab-SJTU/Awesome-LLM-for-Autonomous-Driving-Resources)](https://github.com/Thinklab-SJTU/Awesome-LLM-for-Autonomous-Driving-Resources/blob/main/LICENSE)

This is a collection of research papers about **LLM-for-Autonomous-Driving(LLM4AD)**.
And the repository will be continuously updated to track the frontier of LLM4AD. *Maintained by SJTU-ReThinklab.*

Welcome to follow and star! If you find any related materials could be helpful, feel free to contact us (yangzhenjie@sjtu.edu.cn or jiaxiaosong@sjtu.edu.cn) or make a PR.

## Table of Contents
- [Awesome LLM-for-Autonomous-Driving(LLM4AD)](#awesome-llm-for-autonomous-driving-resources)
  - [Table of Contents](#table-of-contents)
  - [Overview of LLM4AD](#overview-of-llm4ad)
  - [ICLR 2024 Under Review](#iclr-2024-under-review)
  - [Papers](#papers)
  - [Datasets](#datasets)
  - [Citation](#citation)
  - [License](#license)

## Overview of LLM4AD
LLM-for-Autonomous-Driving (LLM4AD) refers to the application of Large Language Models(LLMs) in autonomous driving. We divide existing works based on the perspective of applying LLMs: planning, perception, question answering, and generation. 

![image info](./assets/llm4adpipeline.png)

## Motivation of LLM4AD
The orange circle represents the ideal level of driving competence, akin to that possessed by an experienced human driver. There are two main methods to acquire such proficiency: one, through learning-based techniques within simulated environments; and two, by learning from offline data through similar methodologies. It’s important to note that due to discrepancies between simulations and the real-world, these two domains are not fully the same, i.e. sim2real gap. Concurrently, offline data serves as a subset of real-world data since it’s collected directly from actual surroundings. However, it is difficult to fully cover the distribution as well due to the notorious long-tailed nature of autonomous driving tasks. The final goal of autonomous driving is to elevate driving abilities from a basic green stage to a more advanced blue level through extensive data collection and deep learning.

![image info](./assets/whyllmenhance.png)

## ICLR 2024 Under Review
<details open>
<summary>Toggle</summary>

```
format:
- [title](paper link) [links]
  - task
  - keyword
  - code or project page
  - datasets or environment or simulator
  - summary
```
- [Large Language Models as Decision Makers for Autonomous Driving](https://openreview.net/forum?id=NkYCuGM7E2)
  - Keywords: Large language model, Autonomous driving
  - [Previous summary](#LanguageMPC)

- [DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model](https://openreview.net/forum?id=DUkYDXqxKp)
  - Keywords: Interpretable autonomous driving, large language model, robotics, computer vision
  - [Previous summary](#DriveGPT4)

- [BEV-CLIP: Multi-modal BEV Retrieval Methodology for Complex Scene in Autonomous Driving](https://openreview.net/forum?id=wlqkRFRkYc)
  - Keywords: Autonomous Driving, BEV, Retrieval, Multi-modal, LLM, prompt learning
  - Task: Contrastive learning, Retrieval tasks
  - Datasets: [nuScenes](https://www.nuscenes.org/nuscenes)
  - Summary:
    - Propose a multimodal retrieval method powered by LLM and knowledge graph to achieve contrastive learning between text description and BEV feature retrieval for autonomous driving.

- [LangProp: A code optimization framework using Language Models applied to driving](https://openreview.net/forum?id=UgTrngiN16)
  - Keywords: optimization, autonomous driving, Large Language Models, code generation
  - Task: Code generation, Planning
  - Code: [LangProp](https://github.com/langprop-iclr24/LangProp)
  - Env: [CARLA](https://github.com/carla-simulator)
  - Summary:
    - LangProp is a framework for iteratively optimizing code generated by large language models (LLMs) in a supervised/reinforcement learning setting.

- [GPT-Driver: Learning to Drive with GPT](https://openreview.net/forum?id=SXMTK2eltf)
  - Keywords: Motion Planning, Autonomous Driving, Large Language Models (LLMs), GPT
  - [Previous summary](#GPT-Driver)
  
- [Radar Spectra-language Model for Automotive Scene Parsing](https://openreview.net/forum?id=bdJaYLiOxi)
  - Keywords: radar spectra, radar perception, radar object detection, free space segmentation, autonomous driving, radar classification
  - Task: Detection
  - Datasets: 
    - [RADIal](https://github.com/valeoai/RADIal), [CRUW](https://www.cruwdataset.org/), [nuScenes](https://www.nuscenes.org/nuscenes)
    - For RADIal and CRUW, both images and ground truth labels are used. From nuScenes, only images are taken.
    - Random captions for a frame from CRUW dataset based on ground truth object positions and pseudo ground-truth classes. (not open)
  - Summary:
    - Conduct a benchmark comparison of off-the-shelf vision-language models (VLMs) for classification in automotive scenes.
    - Propose to fine-tune a large VLM specially for automated driving scenes.
  
- [GeoDiffusion: Text-Prompted Geometric Control for Object Detection Data Generation](https://openreview.net/forum?id=xBfQZWeDRH)
  - Keywords: diffusion model, controllable generation, object detection, autonomous driving
  - Task: Detection, Data generation
  - Datasets: [nuScenes](https://www.nuscenes.org/nuscenes), which consists of 60K training samples and 15K validation samples with high-quality bounding box annotations from 10 semantic classes.
  - Summary:
    - propose GEODIFFUSION, an embarrassing simple framework to integrate geometric controls into pre-trained diffusion models for detection data generation via text prompts.
</details>


## Papers
<details open>
<summary>Toggle</summary>

```
format:
- [title](paper link) [links]
  - author1, author2, and author3...
  - publisher
  - task
  - keyword
  - code or project page
  - datasets or environment or simulator
  - publish date
  - summary
  - metrics
```
- [MagicDrive: Street View Generation with Diverse 3D Geometry Control](https://arxiv.org/abs/2310.02601)
  - Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, Qiang Xu
  - Publisher: The Chinese University of Hong Kong, Hong Kong University of Science and Technology, Huawei Noah’s Ark Lab
  - Task: Generation
  - Project: [MagicDrive](https://gaoruiyuan.com/magicdrive/)
  - Code: [MagicDrive](https://github.com/cure-lab/MagicDrive)
  - Datasets: [nuScenes](https://www.nuscenes.org/nuscenes)
  - Publish Date: 2023.10.13
  - Summary:
    - MagicDrive generates highly realistic images, exploiting geometric information from 3D annotations by independently encoding road maps, object boxes, and camera parameters for precise, geometry-guided synthesis. This approach effectively solves the challenge of multi-camera view consistency.
    - It also faces huge challenges in some complex scenes, such as night views and unseen weather conditions.

- [Receive, Reason, and React: Drive as You Say with Large Language Models in Autonomous Vehicles](https://arxiv.org/abs/2310.08034)
  - Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Ziran Wang
  - Publisher: Purdue University,  University of Illinois Urbana-Champaign，University of Virginia，PediaMed.AI.
  - Task: Planning
  - Project: [video](https://www.youtube.com/playlist?list=PLgcRcf9w8BmLJi_fqTGq-7KCZsbpEIE4a)
  - Env: [HighwayEnv](https://github.com/Farama-Foundation/HighwayEnv)
  - Publish Date: 2023.10.12
  - Summary:
    - Utilize LLMs’ linguistic and contextual understanding abilities with specialized tools to integrate the language and reasoning capabilities of LLMs into autonomous vehicles.

- [DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model](https://arxiv.org/abs/2310.07771)
  - Xiaofan Li, Yifu Zhang, Xiaoqing Ye
  - Publisher: Baidu Inc.
  - Task: Generation
  - Project: [official](https://drivingdiffusion.github.io/)
  - Datasets: [nuScenes](https://www.nuscenes.org/nuscenes)
  - Summary:
    - Address the new problem of multi-view video data generation from 3D layout in complex urban scenes.'
    - Propose a generative model DrivingDiffusion to ensure the cross-view, cross-frame consistency and the instance quality of the generated videos.
    - Achieve state-of-the-art video synthesis performance on nuScenes dataset.
  - Metrics:
    - Quality of Generation: Frechet Inception Distance(FID), Frechet Video Distance(FVD)
    - Segmentation Metrics: mIoU

- <a id="LanguageMPC"></a>[LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving](https://arxiv.org/pdf/2310.03026)
  - Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu, Ping Luo, Shengbo Eben Li, Masayoshi Tomizuka, Wei Zhan, Mingyu Ding
  - Publisher: Tsinghua University, The University of Hong Kong, University of California, Berkeley
  - Task: Planning
  - Code: [official](https://sites.google.com/view/llm-mpc)
  - Env: 
    - [ComplexUrbanScenarios](https://github.com/liuyuqi123/ComplexUrbanScenarios)
    - [Carla](https://github.com/carla-simulator)
  - Publish Date: 2023.10.04
  - Summary:
    - Leverage LLMs to provide high-level decisions through chain-of-thought.
    - Convert high-level decisions into mathematical representations to guide the bottom-level controller(MPC).
    - Metrics: Number of failure/collision cases， Inefficiency，time, Penalty

- <a id="DrivingwithLLMs"></a>[Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving](https://browse.arxiv.org/abs/2310.01957)
  - Long Chen, Oleg Sinavski, Jan Hünermann, Alice Karnsund, Andrew James Willmott, Danny Birch, Daniel Maund, Jamie Shotton
  - Publisher: Wayve
  - Task: Planning + VQA
  - Code: [official](https://github.com/wayveai/Driving-with-LLMs)
  - Simulator: a custom-built realistic 2D simulator.(The simulator is not open source.)
  - Datasets: [Driving QA](https://github.com/wayveai/Driving-with-LLMs/tree/main/data), data collection using RL experts in simulator.
  - Publish Date: 2023.10.03
  - Summary:
    - Propose a unique object-level multimodal LLM architecture(Llama2+Lora), using only vectorized representations as input.
    - Develop a new dataset of 160k QA pairs derived from 10k driving scenarios(control commands collected by RL(PPO), QA pair generated by GPT-3.5)
    - Metrics: 
      - Accuracy of traffic light detection
      - MAE for traffic light distance prediction
      - MAE for acceleration
      - MAE for brake pressure
      - MAE for steering wheel angle

- [Talk2BEV: Language-enhanced Bird’s-eye View Maps for Autonomous Driving](https://arxiv.org/abs/2310.02251)
  - Vikrant Dewangan, Tushar Choudhary, Shivam Chandhok, Shubham Priyadarshan, Anushka Jain, Arun K. Singh, Siddharth Srivastava, Krishna Murthy Jatavallabhula, K. Madhava Krishna
  - Publisher: IIIT Hyderabad, University of British Columbia, University of Tartu, TensorTour Inc, MIT
  - Project Page: [official](https://llmbev.github.io/talk2bev/)
  - Code: [Talk2BEV](https://github.com/llmbev/talk2bev)
  - Publish Date: 2023.10.03
  - Summary:
    - Introduces Talk2BEV, a large visionlanguage model (LVLM) interface for bird’s-eye view (BEV) maps in autonomous driving contexts.
    - Does not require any training or finetuning, relying instead on pre-trained image-language models
    - Develop and release Talk2BEV-Bench, a benchmark encom- passing 1000 human-annotated BEV scenarios, with more than 20,000 questions and ground-truth responses from the NuScenes dataset.

- <a id="DriveGPT4"></a>[DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model](https://arxiv.org/abs/2310.01412)
  - Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kenneth K. Y. Wong, Zhenguo Li, Hengshuang Zhao
  - Publisher: The University of Hong Kong, Zhejiang University, Huawei Noah’s Ark Lab, University of Sydney
  - Project Page: [official](https://tonyxuqaq.github.io/projects/DriveGPT4/)
  - Task: Planning + VQA
  - Datasets: 
    - [BDD-X dataset](https://github.com/JinkyuKimUCB/BDD-X-dataset).
  - Publish Date: 2023.10.02
  - Summary:
    - Develop a new visual instruction tuning dataset(based on BDD-X) for interpretable AD assisted by ChatGPT/GPT4.
    - Present a novel multimodal LLM called DriveGPT4(Valley + LLaVA).
  - Metrics: 
    - BLEU4, CIDEr and METETOR, ChatGPT Score.
    - RMSE for control signal prediction.

- <a id="GPT-Driver"></a>[GPT-DRIVER: LEARNING TO DRIVE WITH GPT](https://browse.arxiv.org/abs/2310.01415v1)
  - Jiageng Mao, Yuxi Qian, Hang Zhao, Yue Wang
  - Publisher: University of Southern California, Tsinghua University
  - Task: Planning
  - Datasets: [nuScenes](https://www.nuscenes.org/nuscenes)
  - Publish Date: 2023.10.02
  - Summary:
    - Motion planning as a language modeling problem.
    - Leverage the LLM to generate driving trajectories.
  - Metrics:
    - L2 metric and Collision rate

- [GAIA-1: A Generative World Model for Autonomous Driving](https://arxiv.org/abs/2309.17080)
  - Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, Gianluca Corrado
  - Publisher: Wayve
  - Task: Generation
  - Datasets: 
    - Training dataset consists of 4,700 hours at 25Hz of proprietary driving data collected in London,
UK between 2019 and 2023. It corresponds to approximately 420M unique images. 
    - Validation dataset contains 400 hours of driving data from runs not included in the training set.
    - text coming from either online narration or offline metadata sources
  - Publish Date: 2023.09.29
  - Summary:
    - Introduce GAIA-1, a generative world model that leverages video(pre-trained DINO), text(T5-large), and action inputs to generate realistic driving scenarios.
    - Serve as a valuable neural simulator, allowing the generation of unlimited data.

- [DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models](https://arxiv.org/abs/2309.16292)
  - Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao Ma, Pinlong Cai, Min Dou, Botian Shi, Liang He, Yu Qiao
  - Publisher: Shanghai AI Laboratory, East China Normal University, The Chinese University of Hong Kong
  - Publish Date: 2023.09.28
  - Task: Planning
  - Env: 
    - [HighwayEnv](https://github.com/Farama-Foundation/HighwayEnv)
    - [CitySim](https://github.com/ozheng1993/UCF-SST-CitySim-Dataset), a Drone-Based vehicle trajectory dataset.
  - Summary: 
    - Propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously.

- [SurrealDriver: Designing Generative Driver Agent Simulation Framework in Urban Contexts based on Large Language Model](https://arxiv.org/abs/2309.13193)
  - Ye Jin, Xiaoxi Shen, Huiling Peng, Xiaoan Liu, Jingli Qin, Jiayang Li, Jintao Xie, Peizhong Gao, Guyue Zhou, Jiangtao Gong
  - Keywords: human-AI interaction, driver model, agent, generative AI, large language model, simulation framework
  - Env: [CARLA](https://github.com/carla-simulator)
  - Publisher: Tsinghua University
  - Summary: Propose a generative driver agent simulation framework based on large language models (LLMs), capable of perceiving complex traffic scenarios and providing realistic driving maneuvers.

- [Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles](https://arxiv.org/abs/2309.10228)
  - Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Ziran Wang
  - Publisher: Purdue University, PediaMed.AI Lab, University of Virginia
  - Task: Planning
  - Publish Date: 2023.09.18
  - Summary:
    - Provide a comprehensive framework for integrating Large Language Models (LLMs) into AD.

- [DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving](https://arxiv.org/abs/2309.09777)
  - Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiwen Lu
  - Publisher: GigaAI, Tsinghua University
  - Task: Generation
  - Project Page: [official](https://drivedreamer.github.io/)
  - Datasets: [nuScenes](https://www.nuscenes.org/nuscenes)
  - Publish Date: 2023.09.18
  - Summary:
    - Harness the powerful diffusion model to construct a comprehensive representation of the complex environment.
    - Generate future driving videos and driving policies by a multimodal(text, image, HDMap, Action, 3DBox) world model.

- [Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving](https://arxiv.org/abs/2309.05282)
  - Ali Keysan, Andreas Look, Eitan Kosman, Gonca Gürsun, Jörg Wagner, Yu Yao, Barbara Rakitsch
  - Publisher: Bosch Center for Artificial Intelligence, University of Tubingen, 
  - Task: Prediction
  - Datasets: [nuScenes](https://www.nuscenes.org/nuscenes)
  - Publish Date: 2023.09.13
  - Summary:
    - Integrating pre-trained language models as textbased input encoders for the AD trajectory prediction task.
  - Metrics:
    - minimum Average Displacement Error (minADEk)
    - Final Displacement Error (minFDEk)
    - MissRate over 2 meters

- [TrafficGPT: Viewing, Processing and Interacting with Traffic Foundation Models](https://arxiv.org/abs/2309.06719)
  - Siyao Zhang, Daocheng Fu, Zhao Zhang, Bin Yu, Pinlong Cai
  - Publisher: Beihang University, Key Laboratory of Intelligent Transportation Technology and System,  Shanghai Artificial Intelligence Laboratory
  - Task: Planning
  - Code: [official](https://github.com/lijlansg/TrafficGPT.git)
  - Publish Date: 2023.09.13
  - Summary:
    - Present TrafficGPT—a fusion of ChatGPT and traffic foundation models. 
    - Bridges the critical gap between large language models and traffic foundation models by defining a series of prompts.
  
- [HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving](https://arxiv.org/abs/2309.05186)
  - Xinpeng Ding, Jianhua Han, Hang Xu, Wei Zhang, Xiaomeng Li
  - Publisher: The Hong Kong University of Science and Technology, Huawei Noah’s Ark Lab
  - Task: Detection + VQA
  - Datasets: [DRAMA](https://usa.honda-ri.com/drama)
  - Publish Date: 2023.09.11
  - Summary:
    - Propose HiLM-D (Towards High-Resolution Understanding in MLLMs for Autonomous Driving), an efficient method to incorporate HR information into MLLMs for the ROLISP task.
    - ROLISP that aims to identify, explain and localize the risk object for the ego-vehicle meanwhile predicting its intention and giving suggestions.
  - Metrics:
    - LLM metrics, BLEU4, CIDEr and METETOR, SPICE.
    - Detection metrics, mIoU, IoUs so on.

- <a id="LanguagePrompt"></a>[Language Prompt for Autonomous Driving](https://arxiv.org/abs/2309.04379)
  - Dongming Wu, Wencheng Han, Tiancai Wang, Yingfei Liu, Xiangyu Zhang, Jianbing Shen
  - Publisher: Beijing Institute of Technology, University of Macau, MEGVII Technology, Beijing Academy of Artificial Intelligence
  - Task: Tracking
  - Code: [official](https://github.com/wudongming97/Prompt4Driving)
  - Datasets: NuPrompt(not open), based on [nuScenes](https://www.nuscenes.org/nuscenes). 
  - Publish Date: 2023.09.08
  - Summary:
    - Propose a new large-scale language prompt set(based on nuScenes) for driving scenes, named NuPrompt(3D object-text pairs).
    - Propose an efficient prompt-based tracking model with prompt reasoning modification on PFTrack, called PromptTrack. 

- [MTD-GPT: A Multi-Task Decision-Making GPT Model for Autonomous Driving at Unsignalized Intersections](https://arxiv.org/abs/2307.16118)
  - Jiaqi Liu, Peng Hang, Xiao Qi, Jianqiang Wang, Jian Sun. *ITSC 2023*
  - Publisher: Tongji University, Tsinghua University
  - Task: Prediction
  - Env: [HighwayEnv](https://github.com/Farama-Foundation/HighwayEnv)
  - Publish Date: 2023.07.30
  - Summary:
    - Design a pipeline that leverages RL algorithms to train single-task decision-making experts and utilize expert data.
    - Propose the MTD-GPT model for multi-task(left-turn, straight-through, right-turn) decision-making of AV at unsignalized intersections.

- [Domain Knowledge Distillation from Large Language Model: An Empirical Study in the Autonomous Driving Domain](https://arxiv.org/abs/2307.11769)
  - Yun Tang, Antonio A. Bruto da Costa, Xizhe Zhang, Irvine Patrick, Siddartha Khastgir, Paul Jennings. *ITSC 2023*
  - Publisher: University of Warwick
  - Task: QA
  - Publish Date: 2023.07.17
  - Summary:
    - Develop a web-based distillation assistant enabling supervision and flexible intervention at runtime by prompt engineering and the LLM ChatGPT.

- [Drive Like a Human: Rethinking Autonomous Driving with Large Language Models](https://browse.arxiv.org/abs/2307.07162)
  - Daocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong Cai, Botian Shi, Yu Qiao
  - Publisher: Shanghai AI Lab, East China Normal University
  - Task: Planning
  - Code: [official](https://github.com/PJLab-ADG/DriveLikeAHuman)
  - Env: [HighwayEnv](https://github.com/Farama-Foundation/HighwayEnv)
  - Publish Date: 2023.07.14
  - Summary:
    - Identify three key abilities: Reasoning, Interpretation and Memorization(accumulate experience and self-reflection).
    - Utilize LLM in AD as decision-making to solve long-tail corner cases and increase interpretability.
    - Verify interpretability in closed-loop offline data.

- [Language-Guided Traffic Simulation via Scene-Level Diffusion](https://arxiv.org/abs/2306.06344)
  - Ziyuan Zhong, Davis Rempe, Yuxiao Chen, Boris Ivanovic, Yulong Cao, Danfei Xu, Marco Pavone, Baishakhi Ray
  - Publisher: Columbia University, NVIDIA Research, Stanford University, Georgia Tech
  - Task: Diffusion
  - Publish Date: 2023.07.10
  - Summary: 
    - Present CTG++, a language-guided scene-level conditional diffusion model for realistic query-compliant traffic simulation. 
    - Leverage an LLM for translating a user query into a differentiable loss function and propose a scene-level conditional diffusion model (with a spatial-temporal transformer architecture) to translate the loss function into realistic, query compliant trajectories.

- [ADAPT: Action-aware Driving Caption Transformer](https://arxiv.org/abs/2302.00673)
  - Bu Jin, Xinyu Liu, Yupeng Zheng, Pengfei Li, Hao Zhao, Tong Zhang, Yuhang Zheng, Guyue Zhou, Jingjing Liu **ICRA 2023**
  - Publisher: Chinese Academy of Sciences, Tsinghua University, Peking University, Xidian University, Southern University of Science and Technology, Beihang University
  - Code: [ADAPT](https://github.com/jxbbb/ADAPT)
  - Datasets: [BDD-X dataset](https://github.com/JinkyuKimUCB/BDD-X-dataset)
  - Summary:
    - Propose ADAPT, a new end-to-end transformerbased action narration and reasoning framework for
self-driving vehicles.
    - propose a multi-task joint training framework that aligns both the driving action captioning task and the control signal prediction task.
</details>

## WorkShop
<details open>
<summary>Toggle</summary>

- [Large Language and Vision Models for Autonomous Driving(LLVM-AD) Workshop @ WACV 2024](https://llvm-ad.github.io/)
  - Publisher: Tencent Maps HD Map T.Lab, University of Illinois Urbana- Champaign, Purdue University, University of Virginia
  - Challenge 1: MAPLM: A Large-Scale Vision-Language Dataset for Map and Traffic Scene Understanding
    - Datasets: [Download](https://drive.google.com/drive/folders/1cqFjBH8MLeP6nKFM0l7oV-Srfke-Mx1R?usp=sharing)
    - Task: QA
    - Code: https://github.com/LLVM-AD/MAPLM
    - Description: MAPLM combines point cloud BEV (Bird's Eye View) and panoramic images to provide a rich collection of road scenario images. It includes multi-level scene description data, which helps models navigate through complex and diverse traffic environments.
    - Metric:
      - Frame-overall-accuracy (FRM): A frame is considered correct if all closed-choice questions about it are answered correctly.
      - Question-overall-accuracy (QNS): A question is considered correct if its answer is correct.
      - LAN: How many lanes in current road?
      - INT: Is there any road cross, intersection or lane change zone in the main road?
      - QLT: What is the point cloud data quality in current road area of this image?
      - SCN: What kind of road scene is it in the images? (SCN)    
  - Challenge 2: In-Cabin User Command Understanding (UCU)
    - Datasets: [Download](https://github.com/LLVM-AD/ucu-dataset/blob/main/ucu.csv)
    - Task: QA
    - Code: https://github.com/LLVM-AD/ucu-dataset
    - Description: 
      - This dataset focuses on understanding user commands in the context of autonomous vehicles. It contains 1,099 labeled commands. Each command is a sentence that describes a user’s request to the vehicle. 
    - Metric:
      - Command-level accuracy: A command is considered correctly understood if all eight answers are correct.
      - Question-level accuracy: Evaluation at the individual question level.
</details>

## Datasets
<details open>
<summary>Toggle</summary>

```
format:
- [title](dataset link) [links]
  - author1, author2, and author3...
  - keyword
  - experiment environments or tasks
```
- [Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning](https://arxiv.org/abs/2309.06597)
  - Enna Sachdeva, Nakul Agarwal, Suhas Chundi, Sean Roelofs, Jiachen Li, Behzad Dariush, Chiho Choi, Mykel Kochenderfer
  - Publisher: Honda Research Institute, Stanford University
  - Publish Date: 2023.09.10
  - Summary:
    - A multi-modal ego-centric dataset for Ranking the importance level and Telling the reason for the importance. 
    - Introduce a joint model for joint importance level ranking and natural language captions generation to benchmark our dataset.

- [DriveLM: Drive on Language](https://github.com/OpenDriveLab/DriveLM)
  - Publisher: DriveLM Contributors
  - Dataset: [DriveLM](https://github.com/OpenDriveLab/DriveLM/blob/main/docs/getting_started.md#download-data)
  - Publish Date: 2023.08
  - Summary:
    - Construct dataset based on the nuScenes dataset.
    - Perception questions require the model to recognize objects in the scene. 
    - Prediction questions ask the model to predict the future status of important objects in the scene. 
    - Planning questions prompt the model to give reasonable planning actions and avoid dangerous ones.

- [WEDGE: A multi-weather autonomous driving dataset built from generative vision-language models](https://browse.arxiv.org/abs/2305.07528)
  - Aboli Marathe, Deva Ramanan, Rahee Walambe, Ketan Kotecha. *CVPR 2023*
  - Publisher: Carnegie Mellon University, Symbiosis International University
  - Dataset: [WEDGE](https://github.com/Infernolia/WEDGE)
  - Publish Date: 2023.05.12
  - Summary:
    - A multi-weather autonomous driving dataset built from generative vision-language models.

- [NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario](https://arxiv.org/abs/2305.14836)
  - Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, Yu-Gang Jiang
  - Publisher: Fudan University
  - Dataset: [NuScenes-QA](https://github.com/qiantianwen/NuScenes-QA)
  - Summary:
    - NuScenes-QA provides 459,941 question-answer pairs based on the 34,149 visual scenes, with 376,604 questions from 28,130 scenes used for training, and 83,337 questions from 6,019 scenes used for testing, respectively.
    - The multi-view images and point clouds are first processed by the feature extraction backbone
to obtain BEV features.

- [DRAMA: Joint Risk Localization and Captioning in Driving](https://arxiv.org/abs/2209.10767)
  - Srikanth Malla, Chiho Choi, Isht Dwivedi, Joon Hee Choi, Jiachen Li
  - Publisher: 
  - Datasets: [DRAMA](https://usa.honda-ri.com/drama#Introduction)
  - Summary:
    - Introduce a novel dataset DRAMA that provides linguistic descriptions (with the focus on reasons) of driving risks associated with important objects and that can be used to evaluate a range of visual captioning capabilities in driving scenarios.

- [Language Prompt for Autonomous Driving](https://arxiv.org/abs/2309.04379)
  - Datasets: Nuprompt(Not open)
  - [Previous summary](#LanguagePrompt)

- [Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving](https://browse.arxiv.org/abs/2310.01957)
  - Datasets: [official](https://github.com/wayveai/Driving-with-LLMs/tree/main/data), data collection using RL experts in simulator.
  - [Previous summary](#DrivingwithLLMs)

- [Textual Explanations for Self-Driving Vehicles](https://arxiv.org/abs/1807.11546)
  - Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, Zeynep Akata **ECCV 2018**.
  - Publisher: University of California, Berkeley, Saarland Informatics Campus, University of Amsterdam
  - [BDD-X dataset](https://github.com/JinkyuKimUCB/BDD-X-dataset)

- [Grounding Human-To-Vehicle Advice for Self-Driving Vehicles](https://arxiv.org/abs/1911.06978)
  - Jinkyu Kim, Teruhisa Misu, Yi-Ting Chen, Ashish Tawari, John Canny **CVPR 2019**
  - Publisher: UC Berkeley, Honda Research Institute USA, Inc.
  - [HAD dataset](https://usa.honda-ri.com/had)
</details>

## Citation

If you find our repo is helpful, please consider cite it. (Our survey paper is on the way!)

```BibTeX
@misc{yang2023survey,
      title={A Survey of Large Language Models for Autonomous Driving}, 
      author={Zhenjie Yang and Xiaosong Jia and Hongyang Li and Junchi Yan},
      year={2023},
      eprint={2311.01043},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
```

## License

Awesome LLM for Autonomous Driving Resources is released under the Apache 2.0 license.
